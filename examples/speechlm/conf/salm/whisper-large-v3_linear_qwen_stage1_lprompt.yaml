name: whisper-large-v3_linear_qwen_stage1_lprompt


############ Data ############
data:
  common:
    global_batch_size: 2
    micro_batch_size: 1
    max_seq_length: 4096
    min_seq_length: 1
    sample_rate: 16000
    end_string: null  # additional end string other than <EOS>
    context_key: 'context'  # what key in manifest to load for context
    answer_key: 'answer'  # what key in manifest to load for answer
    prompt_format: null  # prompt formatter to use, set according to the chosen LLM and check for support in nemo/collections/common/prompts
    tokens_to_generate: 128
    add_boa_eoa: false  # whether to add <boa> and <eoa> strings before and after audio features
    prompt_template: "{answer}" # fstring to use for assistant prompt.
    separate_prompt_and_response_with_newline: False 
    truncation_field: 'context' 
    add_bos: false           # <--- handled manually in forward()
    add_eos: true           # <--- handled manually in forward()
    add_sep: false           # <--- ensure nothing spurious gets inserted

  train_ds:
    # Example of how to specify paths to multiple datasets
    # manifest_filepath:
    #   - /path/to/squad.jsonl
    #   - /path/to/mnli.jsonl
    #   - /path/to/boolq.jsonl
    # Example of how each dataset is formatted
    # {'audio_filepath': 'audio1.wav', 'offset': 0.0, 'duration': 12.3, 'context': 'transcribe this audio', 'answer': 'I have a dream...'}
    # the 'answer' field can also be 'text', and a default 'context' field is added if missing in manigests, so as to work with ASR manifests
    manifest_filepath: "/projects_vol/gp_aseschng/peng.yizhou/shared_data/MLC-SLM-Meta/train_manifest_random_speed.jsonl" # Path to a list of JSONL files corresponding to the source data.
    global_batch_size: ${data.common.global_batch_size}
    micro_batch_size: ${data.common.micro_batch_size}
    shuffle: True
    num_workers: 0
    pin_memory: True
    max_seq_length: ${data.common.max_seq_length}
    min_seq_length: ${data.common.min_seq_length}
    drop_last: True
    # Notably, the data weights are controlled by either bucketing_weights
    # or concat_sampling_probabilities depending on the dataset type (tar and
    # non-tar).
    concat_sampling_probabilities: null # When providing a list of datasets, this arg defines the sampling probabilities from each dataset when strategy='random'
    context_key: ${data.common.context_key}
    answer_key: ${data.common.answer_key}
    end_string: ${data.common.end_string}
    add_eos: ${data.common.add_eos}
    add_sep: ${data.common.add_sep}
    add_bos: ${data.common.add_bos}
    separate_prompt_and_response_with_newline: ${data.common.separate_prompt_and_response_with_newline}
    truncation_field: ${data.common.truncation_field} # Options: ['context', 'answer']
    prompt_template: ${data.common.prompt_template} # fstring to use for assistant prompt. Example: "Q: {input}\nA: {output}"
    # ASR configs
    sample_rate: 16000
    max_duration: 24 # it is set for LibriSpeech, you may need to update it for your dataset
    min_duration: 0.1
    # tarred datasets
    is_concat: true
    is_tarred: false
    tarred_audio_filepaths: null
    shuffle_n: 2048
    # bucketing params
    bucketing_strategy: "fully_randomized"
    bucketing_batch_size: null
    prompt_format: ${data.common.prompt_format}
    learnable_prompt: ${model.use_learnable_prompt_tokens}

  validation_ds:
    manifest_filepath: "/projects_vol/gp_aseschng/peng.yizhou/shared_data/MLC-SLM-Meta/dev_manifest.jsonl" # Path to a list of JSONL files corresponding to the source data. Data format is identical to train_ds.
    global_batch_size: ${data.common.global_batch_size}
    micro_batch_size: ${data.common.micro_batch_size}
    shuffle: False
    num_workers: 0
    pin_memory: True
    max_seq_length: ${data.common.max_seq_length}
    min_seq_length: ${data.common.min_seq_length}
    drop_last: true  # no effect, the dataloader will drop last for train and validation anyway
    context_key: ${data.common.context_key}
    answer_key: ${data.common.answer_key}
    add_eos: ${data.common.add_eos}
    end_string: ${data.common.end_string}
    add_sep: ${data.common.add_sep}
    add_bos: ${data.common.add_bos}
    separate_prompt_and_response_with_newline: ${data.common.separate_prompt_and_response_with_newline}
    output_file_path_prefix: null # Prefix of the file to write predictions to.
    truncation_field: ${data.common.truncation_field} # Options: ['context', 'answer']
    index_mapping_dir: null # Path to a directory to write index mapping files.
    prompt_template: ${data.common.prompt_template} # fstring to use for assistant prompt. Example: "Q: {input}\nA: {output}"
    tokens_to_generate: ${data.common.tokens_to_generate}
    write_predictions_to_file: False
    is_concat: true
    # ASR configs
    sample_rate: 16000
    prompt_format: ${data.common.prompt_format}
    learnable_prompt: ${model.use_learnable_prompt_tokens}

    log_every_n_steps: 10
    metric:
      name: "loss" # Name of the evaluation metric to use. Options: ['exact_string_match', 'loss', 'wer', 'bleu', 'rouge']
      average: null # Average the metric over the dataset. Options: ['macro', 'micro']. Works only for 'F1', 'accuracy' etc. Refer to torchmetrics for metrics where this is supported.
      num_classes: null

############ Model ############
model:
  use_learnable_prompt_tokens: true
  learnable_prompt_tokens_len: 20
  freeze_language_model: true
  freeze_speech_model: true
  freeze_modality_adapter: false
  llm:
    pretrained_model: "Qwen/Qwen2.5-7B"
    _target_: nemo.collections.llm.Qwen2Model
    config: 
      _target_: nemo.collections.llm.Qwen25Config7B
    
  speech_encoder:
    # _target_: nemo.collections.speechlm.modules.asr_module.ASRModuleConfig
    use_hf_auto_model: true
    hf_trust_remote_code: false
    hf_load_pretrained_weights: true
    sample_rate: ${data.common.sample_rate}
    pretrained_model: "openai/whisper-large-v3"
    target_module: "model.encoder"
    spec_augment_config:
      _target_: nemo.collections.asr.modules.SpectrogramAugmentation
      freq_masks: 2          # = num_f_mask
      time_masks: 2          # = num_t_mask
      freq_width: 10         # = max_f
      time_width: 50         # = max_t
      rect_masks: 0          # disable SpecCutout
    spec_sub_config:
      _target_: nemo.collections.asr.modules.SpecSubstituteAugmentation
      max_t: 30 # set to zero to disable it
      num_t_sub: 3 # set to zero to disable it

  modality_adapter:
    input_key_from: "config.d_model"  # attribute of model dim in the speech model
    input_key_to: "feat_in"  # attribute of input dim in the modality adapter
    output_key: "feat_out"  # attrubuite of output dim in the modality adapter
    config:
      _target_: nemo.collections.asr.modules.WhisperProjector
      feat_in: -1  # auto-set
      feat_out: -1  # auto-set
      downsample_rate: 4

############ Optimizer ############

optim:
  _target_: nemo.lightning.MegatronOptimizerModule
  config:
    _target_: megatron.core.optimizer.OptimizerConfig
    optimizer: adam
    lr: 1e-4
    clip_grad: 1.0
    weight_decay: 0.0001
  lr_scheduler:
    _target_: nemo.lightning.pytorch.optim.CosineAnnealingScheduler
    max_steps: ${trainer.max_steps}
    warmup_steps: 250
    constant_steps: 10000
    min_lr: 5e-5

############ Trainer ############

# Set this to "DD:HH:MM:SS" format to limit the max time for this job
# If `max_time_per_run` is set, `strategy.ckpt_async_save` must be set to false
max_time_per_run: null

trainer:
  # _target_: nemo.lightning.Trainer
  devices: -1
  accelerator: gpu
  num_nodes: 1
  max_epochs: -1 
  max_steps: 450000 # 1M steps
  accumulate_grad_batches: 1
  log_every_n_steps: 10 # frequency with which training steps are logged 
  val_check_interval: 2000 # If is an int n > 1, will run val every n training steps, if a float 0.0 - 1.0 will run val every epoch fraction, e.g. 0.25 will run val every quarter epoch
  num_sanity_val_steps: 0
  sync_batchnorm: true # used for convolution modules like FC

strategy:
  _target_: nemo.collections.speechlm.strategies.SpeechLMMegatronStrategy
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  context_parallel_size: 1
  ckpt_async_save: true

callbacks:
  checkpoint:
    _target_: nemo.lightning.pytorch.callbacks.ModelCheckpoint
    filename: '${name}--{${callbacks.checkpoint.monitor}:.5f}-{step}'
    monitor: "val_loss"
    mode: "min"
    save_last: true
    save_top_k: 1
    save_weights_only: false
    always_save_context: true

plugins:
  _target_: nemo.lightning.MegatronMixedPrecision
  precision: "bf16-mixed"
  autocast_enabled: null

############ AutoResume ############
resume:
  _target_: nemo.collections.speechlm.utils.resume.SpeechLMAutoResume
  resume_from_directory: null
  resume_from_path: null
  adapter_path: null
  resume_if_exists: true
  resume_past_end: false
  resume_ignore_no_checkpoint: true

############ Logging ############
logger:
  _target_: nemo.lightning.NeMoLogger
  log_dir: null  # default to ./nemo_experiments
  name: ${name}
  wandb:
    _target_: lightning.pytorch.loggers.WandbLogger
    project: null
    name: ${logger.name}
    resume: false

